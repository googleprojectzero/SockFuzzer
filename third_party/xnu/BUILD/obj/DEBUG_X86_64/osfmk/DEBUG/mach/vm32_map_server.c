/*
 * IDENTIFICATION:
 * stub generated Fri Jan  8 14:32:30 2021
 * with a MiG generated by bootstrap_cmds-117
 * OPTIONS: 
 *	KernelServer
 */

/* Module map */

#define	__MIG_check__Request__map_subsystem__ 1

#include "vm32_map_server.h"

#ifndef	mig_internal
#define	mig_internal	static __inline__
#endif	/* mig_internal */

#ifndef	mig_external
#define mig_external
#endif	/* mig_external */

#if	!defined(__MigTypeCheck) && defined(TypeCheck)
#define	__MigTypeCheck		TypeCheck	/* Legacy setting */
#endif	/* !defined(__MigTypeCheck) */

#if	!defined(__MigKernelSpecificCode) && defined(_MIG_KERNEL_SPECIFIC_CODE_)
#define	__MigKernelSpecificCode	_MIG_KERNEL_SPECIFIC_CODE_	/* Legacy setting */
#endif	/* !defined(__MigKernelSpecificCode) */

#ifndef	LimitCheck
#define	LimitCheck 0
#endif	/* LimitCheck */

#ifndef	min
#define	min(a,b)  ( ((a) < (b))? (a): (b) )
#endif	/* min */

#if !defined(_WALIGN_)
#define _WALIGN_(x) (((x) + 3) & ~3)
#endif /* !defined(_WALIGN_) */

#if !defined(_WALIGNSZ_)
#define _WALIGNSZ_(x) _WALIGN_(sizeof(x))
#endif /* !defined(_WALIGNSZ_) */

#ifndef	UseStaticTemplates
#define	UseStaticTemplates	0
#endif	/* UseStaticTemplates */

#ifndef MIG_SERVER_ROUTINE
#define MIG_SERVER_ROUTINE
#endif

#ifndef	__DeclareRcvRpc
#define	__DeclareRcvRpc(_NUM_, _NAME_)
#endif	/* __DeclareRcvRpc */

#ifndef	__BeforeRcvRpc
#define	__BeforeRcvRpc(_NUM_, _NAME_)
#endif	/* __BeforeRcvRpc */

#ifndef	__AfterRcvRpc
#define	__AfterRcvRpc(_NUM_, _NAME_)
#endif	/* __AfterRcvRpc */

#ifndef	__DeclareRcvSimple
#define	__DeclareRcvSimple(_NUM_, _NAME_)
#endif	/* __DeclareRcvSimple */

#ifndef	__BeforeRcvSimple
#define	__BeforeRcvSimple(_NUM_, _NAME_)
#endif	/* __BeforeRcvSimple */

#ifndef	__AfterRcvSimple
#define	__AfterRcvSimple(_NUM_, _NAME_)
#endif	/* __AfterRcvSimple */

#define novalue void

#if	__MigKernelSpecificCode
#define msgh_request_port	msgh_remote_port
#define MACH_MSGH_BITS_REQUEST(bits)	MACH_MSGH_BITS_REMOTE(bits)
#define msgh_reply_port		msgh_local_port
#define MACH_MSGH_BITS_REPLY(bits)	MACH_MSGH_BITS_LOCAL(bits)
#else
#define msgh_request_port	msgh_local_port
#define MACH_MSGH_BITS_REQUEST(bits)	MACH_MSGH_BITS_LOCAL(bits)
#define msgh_reply_port		msgh_remote_port
#define MACH_MSGH_BITS_REPLY(bits)	MACH_MSGH_BITS_REMOTE(bits)
#endif /* __MigKernelSpecificCode */

#define MIG_RETURN_ERROR(X, code)	{\
				((mig_reply_error_t *)X)->RetCode = code;\
				((mig_reply_error_t *)X)->NDR = NDR_record;\
				return;\
				}

/* Forward Declarations */


mig_internal novalue _Xregion
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xallocate
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xdeallocate
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xprotect
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xinherit
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xread
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xread_list
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xwrite
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xcopy
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xread_overwrite
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xmsync
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xbehavior_set
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xmap
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xmachine_attribute
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xremap
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _X_task_wire
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xmake_memory_entry
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xmap_page_query
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xregion_info
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xmapped_pages_info
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xregion_recurse
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xregion_recurse_64
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xregion_info_64
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xregion_64
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xmake_memory_entry_64
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xmap_64
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _Xpurgable_control
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);

mig_internal novalue _X_map_exec_lockdown
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP);


#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__region_t__defined)
#define __MIG_check__Request__region_t__defined

mig_internal kern_return_t __MIG_check__Request__region_t(__attribute__((__unused__)) __Request__region_t *In0P)
{

	typedef __Request__region_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__region_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine region */
mig_internal novalue _Xregion
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t address;
		vm_region_flavor_t flavor;
		mach_msg_type_number_t infoCnt;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__region_t __Request;
	typedef __Reply__region_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__region_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__region_t__defined */

#if	__MigKernelSpecificCode
#if	UseStaticTemplates
	const static mach_msg_port_descriptor_t object_nameTemplate = {
		/* name = */		MACH_PORT_NULL,
		/* pad1 = */		0,
		/* pad2 = */		0,
		/* disp = */		17,
		/* type = */		MACH_MSG_PORT_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#else
#if	UseStaticTemplates
	const static mach_msg_port_descriptor_t object_nameTemplate = {
		/* name = */		MACH_PORT_NULL,
		/* pad1 = */		0,
		/* pad2 = */		0,
		/* disp = */		17,
		/* type = */		MACH_MSG_PORT_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#endif /* __MigKernelSpecificCode */
	kern_return_t RetCode;
	vm_map_t target_task;

	__DeclareRcvRpc(3800, "region")
	__BeforeRcvRpc(3800, "region")

#if	defined(__MIG_check__Request__region_t__defined)
	check_result = __MIG_check__Request__region_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__region_t__defined) */

#if	UseStaticTemplates
	OutP->object_name = object_nameTemplate;
#else	/* UseStaticTemplates */
#if __MigKernelSpecificCode
	OutP->object_name.disposition = 17;
#else
	OutP->object_name.disposition = 17;
#endif /* __MigKernelSpecificCode */
#if !(defined(KERNEL) && defined(__LP64__))
	OutP->object_name.pad1 = 0;
#endif
	OutP->object_name.pad2 = 0;
	OutP->object_name.type = MACH_MSG_PORT_DESCRIPTOR;
#if defined(KERNEL)
	OutP->object_name.pad_end = 0;
#endif
#endif	/* UseStaticTemplates */


	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	OutP->infoCnt = 10;
	if (In0P->infoCnt < OutP->infoCnt)
		OutP->infoCnt = In0P->infoCnt;

	RetCode = vm32_region(target_task, &In0P->address, &OutP->size, In0P->flavor, OutP->info, &OutP->infoCnt, &OutP->object_name.name);
	vm_map_deallocate(target_task);
	if (RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	OutP->address = In0P->address;
	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply) - 40) + (((4 * OutP->infoCnt)));

	OutP->Head.msgh_bits |= MACH_MSGH_BITS_COMPLEX;
	OutP->msgh_body.msgh_descriptor_count = 1;
	__AfterRcvRpc(3800, "region")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__allocate_t__defined)
#define __MIG_check__Request__allocate_t__defined

mig_internal kern_return_t __MIG_check__Request__allocate_t(__attribute__((__unused__)) __Request__allocate_t *In0P)
{

	typedef __Request__allocate_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__allocate_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine allocate */
mig_internal novalue _Xallocate
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t address;
		vm32_size_t size;
		int flags;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__allocate_t __Request;
	typedef __Reply__allocate_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__allocate_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__allocate_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3801, "allocate")
	__BeforeRcvRpc(3801, "allocate")

#if	defined(__MIG_check__Request__allocate_t__defined)
	check_result = __MIG_check__Request__allocate_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__allocate_t__defined) */

	target_task = convert_port_entry_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32_allocate(target_task, &In0P->address, In0P->size, In0P->flags);
	vm_map_deallocate(target_task);
	if (OutP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, OutP->RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	OutP->address = In0P->address;

	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3801, "allocate")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__deallocate_t__defined)
#define __MIG_check__Request__deallocate_t__defined

mig_internal kern_return_t __MIG_check__Request__deallocate_t(__attribute__((__unused__)) __Request__deallocate_t *In0P)
{

	typedef __Request__deallocate_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__deallocate_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine deallocate */
mig_internal novalue _Xdeallocate
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t address;
		vm32_size_t size;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__deallocate_t __Request;
	typedef __Reply__deallocate_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__deallocate_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__deallocate_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3802, "deallocate")
	__BeforeRcvRpc(3802, "deallocate")

#if	defined(__MIG_check__Request__deallocate_t__defined)
	check_result = __MIG_check__Request__deallocate_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__deallocate_t__defined) */

	target_task = convert_port_entry_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32_deallocate(target_task, In0P->address, In0P->size);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	__AfterRcvRpc(3802, "deallocate")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__protect_t__defined)
#define __MIG_check__Request__protect_t__defined

mig_internal kern_return_t __MIG_check__Request__protect_t(__attribute__((__unused__)) __Request__protect_t *In0P)
{

	typedef __Request__protect_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__protect_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine protect */
mig_internal novalue _Xprotect
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t address;
		vm32_size_t size;
		boolean_t set_maximum;
		vm_prot_t new_protection;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__protect_t __Request;
	typedef __Reply__protect_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__protect_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__protect_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3803, "protect")
	__BeforeRcvRpc(3803, "protect")

#if	defined(__MIG_check__Request__protect_t__defined)
	check_result = __MIG_check__Request__protect_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__protect_t__defined) */

	target_task = convert_port_entry_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32_protect(target_task, In0P->address, In0P->size, In0P->set_maximum, In0P->new_protection);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	__AfterRcvRpc(3803, "protect")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__inherit_t__defined)
#define __MIG_check__Request__inherit_t__defined

mig_internal kern_return_t __MIG_check__Request__inherit_t(__attribute__((__unused__)) __Request__inherit_t *In0P)
{

	typedef __Request__inherit_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__inherit_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine inherit */
mig_internal novalue _Xinherit
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t address;
		vm32_size_t size;
		vm_inherit_t new_inheritance;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__inherit_t __Request;
	typedef __Reply__inherit_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__inherit_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__inherit_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3804, "inherit")
	__BeforeRcvRpc(3804, "inherit")

#if	defined(__MIG_check__Request__inherit_t__defined)
	check_result = __MIG_check__Request__inherit_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__inherit_t__defined) */

	target_task = convert_port_entry_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32_inherit(target_task, In0P->address, In0P->size, In0P->new_inheritance);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	__AfterRcvRpc(3804, "inherit")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__read_t__defined)
#define __MIG_check__Request__read_t__defined

mig_internal kern_return_t __MIG_check__Request__read_t(__attribute__((__unused__)) __Request__read_t *In0P)
{

	typedef __Request__read_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__read_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine read */
mig_internal novalue _Xread
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t address;
		vm32_size_t size;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__read_t __Request;
	typedef __Reply__read_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__read_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__read_t__defined */

#if	__MigKernelSpecificCode
#if	UseStaticTemplates
	const static mach_msg_ool_descriptor_t dataTemplate = {
		/* addr = */		(void *)0,
		/* size = */		0,
		/* deal = */		FALSE,
		/* copy = */		MACH_MSG_VIRTUAL_COPY,
		/* pad2 = */		0,
		/* type = */		MACH_MSG_OOL_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#else
#if	UseStaticTemplates
	const static mach_msg_ool_descriptor_t dataTemplate = {
		/* addr = */		(void *)0,
		/* size = */		0,
		/* deal = */		FALSE,
		/* copy = */		MACH_MSG_VIRTUAL_COPY,
		/* pad2 = */		0,
		/* type = */		MACH_MSG_OOL_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#endif /* __MigKernelSpecificCode */
	kern_return_t RetCode;
	vm_map_t target_task;

	__DeclareRcvRpc(3805, "read")
	__BeforeRcvRpc(3805, "read")

#if	defined(__MIG_check__Request__read_t__defined)
	check_result = __MIG_check__Request__read_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__read_t__defined) */

#if	UseStaticTemplates
	OutP->data = dataTemplate;
#else	/* UseStaticTemplates */
	OutP->data.deallocate =  FALSE;
	OutP->data.copy = MACH_MSG_VIRTUAL_COPY;
	OutP->data.pad1 = 0;
	OutP->data.type = MACH_MSG_OOL_DESCRIPTOR;
#if defined(KERNEL) && !defined(__LP64__)
	OutP->data.pad_end = 0;
#endif
#endif	/* UseStaticTemplates */


	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	RetCode = vm32_read(target_task, In0P->address, In0P->size, (vm_offset_t *)&(OutP->data.address), &OutP->dataCnt);
	vm_map_deallocate(target_task);
	if (RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */
	OutP->data.size = OutP->dataCnt;


	OutP->NDR = NDR_record;


	OutP->Head.msgh_bits |= MACH_MSGH_BITS_COMPLEX;
	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	OutP->msgh_body.msgh_descriptor_count = 1;
	__AfterRcvRpc(3805, "read")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__read_list_t__defined)
#define __MIG_check__Request__read_list_t__defined

mig_internal kern_return_t __MIG_check__Request__read_list_t(__attribute__((__unused__)) __Request__read_list_t *In0P)
{

	typedef __Request__read_list_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__read_list_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine read_list */
mig_internal novalue _Xread_list
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_read_entry_t data_list;
		natural_t count;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__read_list_t __Request;
	typedef __Reply__read_list_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__read_list_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__read_list_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3806, "read_list")
	__BeforeRcvRpc(3806, "read_list")

#if	defined(__MIG_check__Request__read_list_t__defined)
	check_result = __MIG_check__Request__read_list_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__read_list_t__defined) */

	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32_read_list(target_task, In0P->data_list, In0P->count);
	vm_map_deallocate(target_task);
	if (OutP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, OutP->RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	{   typedef struct { char data[2048]; } *sp;
	    * (sp) OutP->data_list = * (sp) In0P->data_list;
	}

	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3806, "read_list")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__write_t__defined)
#define __MIG_check__Request__write_t__defined

mig_internal kern_return_t __MIG_check__Request__write_t(__attribute__((__unused__)) __Request__write_t *In0P)
{

	typedef __Request__write_t __Request;
#if	__MigTypeCheck
	if (!(In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->msgh_body.msgh_descriptor_count != 1) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

#if	__MigTypeCheck
	if (In0P->data.type != MACH_MSG_OOL_DESCRIPTOR)
		return MIG_TYPE_ERROR;
#endif	/* __MigTypeCheck */

#if __MigTypeCheck
	if (In0P->data.size != In0P->dataCnt)
		return MIG_TYPE_ERROR;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__write_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine write */
mig_internal novalue _Xwrite
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		/* start of the kernel processed data */
		mach_msg_body_t msgh_body;
		mach_msg_ool_descriptor_t data;
		/* end of the kernel processed data */
		NDR_record_t NDR;
		vm32_address_t address;
		mach_msg_type_number_t dataCnt;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__write_t __Request;
	typedef __Reply__write_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__write_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__write_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3807, "write")
	__BeforeRcvRpc(3807, "write")

#if	defined(__MIG_check__Request__write_t__defined)
	check_result = __MIG_check__Request__write_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__write_t__defined) */

	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32_write(target_task, In0P->address, (vm_offset_t)(In0P->data.address), In0P->data.size);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	__AfterRcvRpc(3807, "write")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__copy_t__defined)
#define __MIG_check__Request__copy_t__defined

mig_internal kern_return_t __MIG_check__Request__copy_t(__attribute__((__unused__)) __Request__copy_t *In0P)
{

	typedef __Request__copy_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__copy_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine copy */
mig_internal novalue _Xcopy
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t source_address;
		vm32_size_t size;
		vm32_address_t dest_address;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__copy_t __Request;
	typedef __Reply__copy_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__copy_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__copy_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3808, "copy")
	__BeforeRcvRpc(3808, "copy")

#if	defined(__MIG_check__Request__copy_t__defined)
	check_result = __MIG_check__Request__copy_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__copy_t__defined) */

	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32_copy(target_task, In0P->source_address, In0P->size, In0P->dest_address);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	__AfterRcvRpc(3808, "copy")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__read_overwrite_t__defined)
#define __MIG_check__Request__read_overwrite_t__defined

mig_internal kern_return_t __MIG_check__Request__read_overwrite_t(__attribute__((__unused__)) __Request__read_overwrite_t *In0P)
{

	typedef __Request__read_overwrite_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__read_overwrite_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine read_overwrite */
mig_internal novalue _Xread_overwrite
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t address;
		vm32_size_t size;
		vm32_address_t data;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__read_overwrite_t __Request;
	typedef __Reply__read_overwrite_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__read_overwrite_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__read_overwrite_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3809, "read_overwrite")
	__BeforeRcvRpc(3809, "read_overwrite")

#if	defined(__MIG_check__Request__read_overwrite_t__defined)
	check_result = __MIG_check__Request__read_overwrite_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__read_overwrite_t__defined) */

	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32_read_overwrite(target_task, In0P->address, In0P->size, In0P->data, &OutP->outsize);
	vm_map_deallocate(target_task);
	if (OutP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, OutP->RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3809, "read_overwrite")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__msync_t__defined)
#define __MIG_check__Request__msync_t__defined

mig_internal kern_return_t __MIG_check__Request__msync_t(__attribute__((__unused__)) __Request__msync_t *In0P)
{

	typedef __Request__msync_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__msync_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine msync */
mig_internal novalue _Xmsync
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t address;
		vm32_size_t size;
		vm_sync_t sync_flags;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__msync_t __Request;
	typedef __Reply__msync_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__msync_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__msync_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3810, "msync")
	__BeforeRcvRpc(3810, "msync")

#if	defined(__MIG_check__Request__msync_t__defined)
	check_result = __MIG_check__Request__msync_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__msync_t__defined) */

	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32_msync(target_task, In0P->address, In0P->size, In0P->sync_flags);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	__AfterRcvRpc(3810, "msync")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__behavior_set_t__defined)
#define __MIG_check__Request__behavior_set_t__defined

mig_internal kern_return_t __MIG_check__Request__behavior_set_t(__attribute__((__unused__)) __Request__behavior_set_t *In0P)
{

	typedef __Request__behavior_set_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__behavior_set_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine behavior_set */
mig_internal novalue _Xbehavior_set
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t address;
		vm32_size_t size;
		vm_behavior_t new_behavior;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__behavior_set_t __Request;
	typedef __Reply__behavior_set_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__behavior_set_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__behavior_set_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3811, "behavior_set")
	__BeforeRcvRpc(3811, "behavior_set")

#if	defined(__MIG_check__Request__behavior_set_t__defined)
	check_result = __MIG_check__Request__behavior_set_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__behavior_set_t__defined) */

	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32_behavior_set(target_task, In0P->address, In0P->size, In0P->new_behavior);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	__AfterRcvRpc(3811, "behavior_set")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__map_t__defined)
#define __MIG_check__Request__map_t__defined

mig_internal kern_return_t __MIG_check__Request__map_t(__attribute__((__unused__)) __Request__map_t *In0P)
{

	typedef __Request__map_t __Request;
#if	__MigTypeCheck
	if (!(In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->msgh_body.msgh_descriptor_count != 1) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

#if	__MigTypeCheck
	if (In0P->object.type != MACH_MSG_PORT_DESCRIPTOR ||
	    In0P->object.disposition != 17)
		return MIG_TYPE_ERROR;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__map_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine map */
mig_internal novalue _Xmap
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		/* start of the kernel processed data */
		mach_msg_body_t msgh_body;
		mach_msg_port_descriptor_t object;
		/* end of the kernel processed data */
		NDR_record_t NDR;
		vm32_address_t address;
		vm32_size_t size;
		vm32_address_t mask;
		int flags;
		vm32_offset_t offset;
		boolean_t copy;
		vm_prot_t cur_protection;
		vm_prot_t max_protection;
		vm_inherit_t inheritance;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__map_t __Request;
	typedef __Reply__map_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__map_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__map_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3812, "map")
	__BeforeRcvRpc(3812, "map")

#if	defined(__MIG_check__Request__map_t__defined)
	check_result = __MIG_check__Request__map_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__map_t__defined) */

	target_task = convert_port_entry_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32_map(target_task, &In0P->address, In0P->size, In0P->mask, In0P->flags, null_conversion(In0P->object.name), In0P->offset, In0P->copy, In0P->cur_protection, In0P->max_protection, In0P->inheritance);
	vm_map_deallocate(target_task);
	if (OutP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, OutP->RetCode);
	}
#if	__MigKernelSpecificCode

	if (IP_VALID((ipc_port_t)In0P->object.name))
		ipc_port_release_send((ipc_port_t)In0P->object.name);
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	OutP->address = In0P->address;

	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3812, "map")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__machine_attribute_t__defined)
#define __MIG_check__Request__machine_attribute_t__defined

mig_internal kern_return_t __MIG_check__Request__machine_attribute_t(__attribute__((__unused__)) __Request__machine_attribute_t *In0P)
{

	typedef __Request__machine_attribute_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__machine_attribute_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine machine_attribute */
mig_internal novalue _Xmachine_attribute
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t address;
		vm32_size_t size;
		vm_machine_attribute_t attribute;
		vm_machine_attribute_val_t value;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__machine_attribute_t __Request;
	typedef __Reply__machine_attribute_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__machine_attribute_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__machine_attribute_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3813, "machine_attribute")
	__BeforeRcvRpc(3813, "machine_attribute")

#if	defined(__MIG_check__Request__machine_attribute_t__defined)
	check_result = __MIG_check__Request__machine_attribute_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__machine_attribute_t__defined) */

	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32_machine_attribute(target_task, In0P->address, In0P->size, In0P->attribute, &In0P->value);
	vm_map_deallocate(target_task);
	if (OutP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, OutP->RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	OutP->value = In0P->value;

	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3813, "machine_attribute")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__remap_t__defined)
#define __MIG_check__Request__remap_t__defined

mig_internal kern_return_t __MIG_check__Request__remap_t(__attribute__((__unused__)) __Request__remap_t *In0P)
{

	typedef __Request__remap_t __Request;
#if	__MigTypeCheck
	if (!(In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->msgh_body.msgh_descriptor_count != 1) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

#if	__MigTypeCheck
	if (In0P->src_task.type != MACH_MSG_PORT_DESCRIPTOR ||
	    In0P->src_task.disposition != 17)
		return MIG_TYPE_ERROR;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__remap_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine remap */
mig_internal novalue _Xremap
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		/* start of the kernel processed data */
		mach_msg_body_t msgh_body;
		mach_msg_port_descriptor_t src_task;
		/* end of the kernel processed data */
		NDR_record_t NDR;
		vm32_address_t target_address;
		vm32_size_t size;
		vm32_address_t mask;
		boolean_t anywhere;
		vm32_address_t src_address;
		boolean_t copy;
		vm_inherit_t inheritance;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__remap_t __Request;
	typedef __Reply__remap_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__remap_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__remap_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;
	vm_map_t src_task;

	__DeclareRcvRpc(3814, "remap")
	__BeforeRcvRpc(3814, "remap")

#if	defined(__MIG_check__Request__remap_t__defined)
	check_result = __MIG_check__Request__remap_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__remap_t__defined) */

	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	src_task = convert_port_to_map(In0P->src_task.name);

	OutP->RetCode = vm32_remap(target_task, &In0P->target_address, In0P->size, In0P->mask, In0P->anywhere, src_task, In0P->src_address, In0P->copy, &OutP->cur_protection, &OutP->max_protection, In0P->inheritance);
	vm_map_deallocate(src_task);
	vm_map_deallocate(target_task);
	if (OutP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, OutP->RetCode);
	}
#if	__MigKernelSpecificCode

	if (IP_VALID((ipc_port_t)In0P->src_task.name))
		ipc_port_release_send((ipc_port_t)In0P->src_task.name);
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	OutP->target_address = In0P->target_address;

	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3814, "remap")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request___task_wire_t__defined)
#define __MIG_check__Request___task_wire_t__defined

mig_internal kern_return_t __MIG_check__Request___task_wire_t(__attribute__((__unused__)) __Request___task_wire_t *In0P)
{

	typedef __Request___task_wire_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request___task_wire_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine _task_wire */
mig_internal novalue _X_task_wire
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		boolean_t must_wire;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request___task_wire_t __Request;
	typedef __Reply___task_wire_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request___task_wire_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request___task_wire_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3815, "_task_wire")
	__BeforeRcvRpc(3815, "_task_wire")

#if	defined(__MIG_check__Request___task_wire_t__defined)
	check_result = __MIG_check__Request___task_wire_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request___task_wire_t__defined) */

	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32__task_wire(target_task, In0P->must_wire);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	__AfterRcvRpc(3815, "_task_wire")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__make_memory_entry_t__defined)
#define __MIG_check__Request__make_memory_entry_t__defined

mig_internal kern_return_t __MIG_check__Request__make_memory_entry_t(__attribute__((__unused__)) __Request__make_memory_entry_t *In0P)
{

	typedef __Request__make_memory_entry_t __Request;
#if	__MigTypeCheck
	if (!(In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->msgh_body.msgh_descriptor_count != 1) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

#if	__MigTypeCheck
	if (In0P->parent_entry.type != MACH_MSG_PORT_DESCRIPTOR ||
	    In0P->parent_entry.disposition != 17)
		return MIG_TYPE_ERROR;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__make_memory_entry_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine make_memory_entry */
mig_internal novalue _Xmake_memory_entry
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		/* start of the kernel processed data */
		mach_msg_body_t msgh_body;
		mach_msg_port_descriptor_t parent_entry;
		/* end of the kernel processed data */
		NDR_record_t NDR;
		vm32_size_t size;
		vm32_offset_t offset;
		vm_prot_t permission;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__make_memory_entry_t __Request;
	typedef __Reply__make_memory_entry_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__make_memory_entry_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__make_memory_entry_t__defined */

#if	__MigKernelSpecificCode
#if	UseStaticTemplates
	const static mach_msg_port_descriptor_t object_handleTemplate = {
		/* name = */		MACH_PORT_NULL,
		/* pad1 = */		0,
		/* pad2 = */		0,
		/* disp = */		17,
		/* type = */		MACH_MSG_PORT_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#else
#if	UseStaticTemplates
	const static mach_msg_port_descriptor_t object_handleTemplate = {
		/* name = */		MACH_PORT_NULL,
		/* pad1 = */		0,
		/* pad2 = */		0,
		/* disp = */		17,
		/* type = */		MACH_MSG_PORT_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#endif /* __MigKernelSpecificCode */
	kern_return_t RetCode;
	vm_map_t target_task;
	mem_entry_name_port_t object_handle;

	__DeclareRcvRpc(3816, "make_memory_entry")
	__BeforeRcvRpc(3816, "make_memory_entry")

#if	defined(__MIG_check__Request__make_memory_entry_t__defined)
	check_result = __MIG_check__Request__make_memory_entry_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__make_memory_entry_t__defined) */

#if	UseStaticTemplates
	OutP->object_handle = object_handleTemplate;
#else	/* UseStaticTemplates */
#if __MigKernelSpecificCode
	OutP->object_handle.disposition = 17;
#else
	OutP->object_handle.disposition = 17;
#endif /* __MigKernelSpecificCode */
#if !(defined(KERNEL) && defined(__LP64__))
	OutP->object_handle.pad1 = 0;
#endif
	OutP->object_handle.pad2 = 0;
	OutP->object_handle.type = MACH_MSG_PORT_DESCRIPTOR;
#if defined(KERNEL)
	OutP->object_handle.pad_end = 0;
#endif
#endif	/* UseStaticTemplates */


	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	RetCode = vm32_make_memory_entry(target_task, &In0P->size, In0P->offset, In0P->permission, &object_handle, null_conversion(In0P->parent_entry.name));
	vm_map_deallocate(target_task);
	if (RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, RetCode);
	}
#if	__MigKernelSpecificCode

	if (IP_VALID((ipc_port_t)In0P->parent_entry.name))
		ipc_port_release_send((ipc_port_t)In0P->parent_entry.name);
#endif /* __MigKernelSpecificCode */
	OutP->object_handle.name = (mach_port_t)null_conversion(object_handle);


	OutP->NDR = NDR_record;


	OutP->size = In0P->size;

	OutP->Head.msgh_bits |= MACH_MSGH_BITS_COMPLEX;
	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	OutP->msgh_body.msgh_descriptor_count = 1;
	__AfterRcvRpc(3816, "make_memory_entry")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__map_page_query_t__defined)
#define __MIG_check__Request__map_page_query_t__defined

mig_internal kern_return_t __MIG_check__Request__map_page_query_t(__attribute__((__unused__)) __Request__map_page_query_t *In0P)
{

	typedef __Request__map_page_query_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__map_page_query_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine map_page_query */
mig_internal novalue _Xmap_page_query
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_offset_t offset;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__map_page_query_t __Request;
	typedef __Reply__map_page_query_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__map_page_query_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__map_page_query_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_map;

	__DeclareRcvRpc(3817, "map_page_query")
	__BeforeRcvRpc(3817, "map_page_query")

#if	defined(__MIG_check__Request__map_page_query_t__defined)
	check_result = __MIG_check__Request__map_page_query_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__map_page_query_t__defined) */

	target_map = convert_port_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32_map_page_query(target_map, In0P->offset, &OutP->disposition, &OutP->ref_count);
	vm_map_deallocate(target_map);
	if (OutP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, OutP->RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3817, "map_page_query")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__region_info_t__defined)
#define __MIG_check__Request__region_info_t__defined

mig_internal kern_return_t __MIG_check__Request__region_info_t(__attribute__((__unused__)) __Request__region_info_t *In0P)
{

	typedef __Request__region_info_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__region_info_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine region_info */
mig_internal novalue _Xregion_info
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t address;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__region_info_t __Request;
	typedef __Reply__region_info_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__region_info_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__region_info_t__defined */

#if	__MigKernelSpecificCode
#if	UseStaticTemplates
	const static mach_msg_ool_descriptor_t objectsTemplate = {
		/* addr = */		(void *)0,
		/* size = */		0,
		/* deal = */		FALSE,
		/* copy = */		MACH_MSG_VIRTUAL_COPY,
		/* pad2 = */		0,
		/* type = */		MACH_MSG_OOL_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#else
#if	UseStaticTemplates
	const static mach_msg_ool_descriptor_t objectsTemplate = {
		/* addr = */		(void *)0,
		/* size = */		0,
		/* deal = */		FALSE,
		/* copy = */		MACH_MSG_VIRTUAL_COPY,
		/* pad2 = */		0,
		/* type = */		MACH_MSG_OOL_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#endif /* __MigKernelSpecificCode */
	kern_return_t RetCode;
	vm_map_t task;

	__DeclareRcvRpc(3818, "region_info")
	__BeforeRcvRpc(3818, "region_info")

#if	defined(__MIG_check__Request__region_info_t__defined)
	check_result = __MIG_check__Request__region_info_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__region_info_t__defined) */

#if	UseStaticTemplates
	OutP->objects = objectsTemplate;
#else	/* UseStaticTemplates */
	OutP->objects.deallocate =  FALSE;
	OutP->objects.copy = MACH_MSG_VIRTUAL_COPY;
	OutP->objects.pad1 = 0;
	OutP->objects.type = MACH_MSG_OOL_DESCRIPTOR;
#if defined(KERNEL) && !defined(__LP64__)
	OutP->objects.pad_end = 0;
#endif
#endif	/* UseStaticTemplates */


	task = convert_port_to_map(In0P->Head.msgh_request_port);

	RetCode = vm32_region_info(task, In0P->address, &OutP->region, (vm_info_object_array_t *)&(OutP->objects.address), &OutP->objectsCnt);
	vm_map_deallocate(task);
	if (RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */
	OutP->objects.size = OutP->objectsCnt * 84;


	OutP->NDR = NDR_record;


	OutP->Head.msgh_bits |= MACH_MSGH_BITS_COMPLEX;
	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	OutP->msgh_body.msgh_descriptor_count = 1;
	__AfterRcvRpc(3818, "region_info")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__mapped_pages_info_t__defined)
#define __MIG_check__Request__mapped_pages_info_t__defined

mig_internal kern_return_t __MIG_check__Request__mapped_pages_info_t(__attribute__((__unused__)) __Request__mapped_pages_info_t *In0P)
{

	typedef __Request__mapped_pages_info_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__mapped_pages_info_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine mapped_pages_info */
mig_internal novalue _Xmapped_pages_info
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__mapped_pages_info_t __Request;
	typedef __Reply__mapped_pages_info_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__mapped_pages_info_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__mapped_pages_info_t__defined */

#if	__MigKernelSpecificCode
#if	UseStaticTemplates
	const static mach_msg_ool_descriptor_t pagesTemplate = {
		/* addr = */		(void *)0,
		/* size = */		0,
		/* deal = */		FALSE,
		/* copy = */		MACH_MSG_VIRTUAL_COPY,
		/* pad2 = */		0,
		/* type = */		MACH_MSG_OOL_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#else
#if	UseStaticTemplates
	const static mach_msg_ool_descriptor_t pagesTemplate = {
		/* addr = */		(void *)0,
		/* size = */		0,
		/* deal = */		FALSE,
		/* copy = */		MACH_MSG_VIRTUAL_COPY,
		/* pad2 = */		0,
		/* type = */		MACH_MSG_OOL_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#endif /* __MigKernelSpecificCode */
	kern_return_t RetCode;
	vm_map_t task;

	__DeclareRcvRpc(3819, "mapped_pages_info")
	__BeforeRcvRpc(3819, "mapped_pages_info")

#if	defined(__MIG_check__Request__mapped_pages_info_t__defined)
	check_result = __MIG_check__Request__mapped_pages_info_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__mapped_pages_info_t__defined) */

#if	UseStaticTemplates
	OutP->pages = pagesTemplate;
#else	/* UseStaticTemplates */
	OutP->pages.deallocate =  FALSE;
	OutP->pages.copy = MACH_MSG_VIRTUAL_COPY;
	OutP->pages.pad1 = 0;
	OutP->pages.type = MACH_MSG_OOL_DESCRIPTOR;
#if defined(KERNEL) && !defined(__LP64__)
	OutP->pages.pad_end = 0;
#endif
#endif	/* UseStaticTemplates */


	task = convert_port_to_map(In0P->Head.msgh_request_port);

	RetCode = vm32_mapped_pages_info(task, (page_address_array_t *)&(OutP->pages.address), &OutP->pagesCnt);
	vm_map_deallocate(task);
	if (RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */
	OutP->pages.size = OutP->pagesCnt * 4;


	OutP->NDR = NDR_record;


	OutP->Head.msgh_bits |= MACH_MSGH_BITS_COMPLEX;
	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	OutP->msgh_body.msgh_descriptor_count = 1;
	__AfterRcvRpc(3819, "mapped_pages_info")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__region_recurse_t__defined)
#define __MIG_check__Request__region_recurse_t__defined

mig_internal kern_return_t __MIG_check__Request__region_recurse_t(__attribute__((__unused__)) __Request__region_recurse_t *In0P)
{

	typedef __Request__region_recurse_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__region_recurse_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine region_recurse */
mig_internal novalue _Xregion_recurse
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t address;
		natural_t nesting_depth;
		mach_msg_type_number_t infoCnt;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__region_recurse_t __Request;
	typedef __Reply__region_recurse_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__region_recurse_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__region_recurse_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3821, "region_recurse")
	__BeforeRcvRpc(3821, "region_recurse")

#if	defined(__MIG_check__Request__region_recurse_t__defined)
	check_result = __MIG_check__Request__region_recurse_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__region_recurse_t__defined) */

	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	OutP->infoCnt = 19;
	if (In0P->infoCnt < OutP->infoCnt)
		OutP->infoCnt = In0P->infoCnt;

	OutP->RetCode = vm32_region_recurse(target_task, &In0P->address, &OutP->size, &In0P->nesting_depth, OutP->info, &OutP->infoCnt);
	vm_map_deallocate(target_task);
	if (OutP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, OutP->RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	OutP->address = In0P->address;

	OutP->nesting_depth = In0P->nesting_depth;
	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply) - 76) + (((4 * OutP->infoCnt)));

	__AfterRcvRpc(3821, "region_recurse")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__region_recurse_64_t__defined)
#define __MIG_check__Request__region_recurse_64_t__defined

mig_internal kern_return_t __MIG_check__Request__region_recurse_64_t(__attribute__((__unused__)) __Request__region_recurse_64_t *In0P)
{

	typedef __Request__region_recurse_64_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__region_recurse_64_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine region_recurse_64 */
mig_internal novalue _Xregion_recurse_64
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t address;
		natural_t nesting_depth;
		mach_msg_type_number_t infoCnt;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__region_recurse_64_t __Request;
	typedef __Reply__region_recurse_64_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__region_recurse_64_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__region_recurse_64_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3822, "region_recurse_64")
	__BeforeRcvRpc(3822, "region_recurse_64")

#if	defined(__MIG_check__Request__region_recurse_64_t__defined)
	check_result = __MIG_check__Request__region_recurse_64_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__region_recurse_64_t__defined) */

	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	OutP->infoCnt = 19;
	if (In0P->infoCnt < OutP->infoCnt)
		OutP->infoCnt = In0P->infoCnt;

	OutP->RetCode = vm32_region_recurse_64(target_task, &In0P->address, &OutP->size, &In0P->nesting_depth, OutP->info, &OutP->infoCnt);
	vm_map_deallocate(target_task);
	if (OutP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, OutP->RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	OutP->address = In0P->address;

	OutP->nesting_depth = In0P->nesting_depth;
	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply) - 76) + (((4 * OutP->infoCnt)));

	__AfterRcvRpc(3822, "region_recurse_64")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__region_info_64_t__defined)
#define __MIG_check__Request__region_info_64_t__defined

mig_internal kern_return_t __MIG_check__Request__region_info_64_t(__attribute__((__unused__)) __Request__region_info_64_t *In0P)
{

	typedef __Request__region_info_64_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__region_info_64_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine region_info_64 */
mig_internal novalue _Xregion_info_64
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t address;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__region_info_64_t __Request;
	typedef __Reply__region_info_64_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__region_info_64_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__region_info_64_t__defined */

#if	__MigKernelSpecificCode
#if	UseStaticTemplates
	const static mach_msg_ool_descriptor_t objectsTemplate = {
		/* addr = */		(void *)0,
		/* size = */		0,
		/* deal = */		FALSE,
		/* copy = */		MACH_MSG_VIRTUAL_COPY,
		/* pad2 = */		0,
		/* type = */		MACH_MSG_OOL_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#else
#if	UseStaticTemplates
	const static mach_msg_ool_descriptor_t objectsTemplate = {
		/* addr = */		(void *)0,
		/* size = */		0,
		/* deal = */		FALSE,
		/* copy = */		MACH_MSG_VIRTUAL_COPY,
		/* pad2 = */		0,
		/* type = */		MACH_MSG_OOL_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#endif /* __MigKernelSpecificCode */
	kern_return_t RetCode;
	vm_map_t task;

	__DeclareRcvRpc(3823, "region_info_64")
	__BeforeRcvRpc(3823, "region_info_64")

#if	defined(__MIG_check__Request__region_info_64_t__defined)
	check_result = __MIG_check__Request__region_info_64_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__region_info_64_t__defined) */

#if	UseStaticTemplates
	OutP->objects = objectsTemplate;
#else	/* UseStaticTemplates */
	OutP->objects.deallocate =  FALSE;
	OutP->objects.copy = MACH_MSG_VIRTUAL_COPY;
	OutP->objects.pad1 = 0;
	OutP->objects.type = MACH_MSG_OOL_DESCRIPTOR;
#if defined(KERNEL) && !defined(__LP64__)
	OutP->objects.pad_end = 0;
#endif
#endif	/* UseStaticTemplates */


	task = convert_port_to_map(In0P->Head.msgh_request_port);

	RetCode = vm32_region_info_64(task, In0P->address, &OutP->region, (vm_info_object_array_t *)&(OutP->objects.address), &OutP->objectsCnt);
	vm_map_deallocate(task);
	if (RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */
	OutP->objects.size = OutP->objectsCnt * 84;


	OutP->NDR = NDR_record;


	OutP->Head.msgh_bits |= MACH_MSGH_BITS_COMPLEX;
	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	OutP->msgh_body.msgh_descriptor_count = 1;
	__AfterRcvRpc(3823, "region_info_64")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__region_64_t__defined)
#define __MIG_check__Request__region_64_t__defined

mig_internal kern_return_t __MIG_check__Request__region_64_t(__attribute__((__unused__)) __Request__region_64_t *In0P)
{

	typedef __Request__region_64_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__region_64_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine region_64 */
mig_internal novalue _Xregion_64
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t address;
		vm_region_flavor_t flavor;
		mach_msg_type_number_t infoCnt;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__region_64_t __Request;
	typedef __Reply__region_64_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__region_64_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__region_64_t__defined */

#if	__MigKernelSpecificCode
#if	UseStaticTemplates
	const static mach_msg_port_descriptor_t object_nameTemplate = {
		/* name = */		MACH_PORT_NULL,
		/* pad1 = */		0,
		/* pad2 = */		0,
		/* disp = */		17,
		/* type = */		MACH_MSG_PORT_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#else
#if	UseStaticTemplates
	const static mach_msg_port_descriptor_t object_nameTemplate = {
		/* name = */		MACH_PORT_NULL,
		/* pad1 = */		0,
		/* pad2 = */		0,
		/* disp = */		17,
		/* type = */		MACH_MSG_PORT_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#endif /* __MigKernelSpecificCode */
	kern_return_t RetCode;
	vm_map_t target_task;

	__DeclareRcvRpc(3824, "region_64")
	__BeforeRcvRpc(3824, "region_64")

#if	defined(__MIG_check__Request__region_64_t__defined)
	check_result = __MIG_check__Request__region_64_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__region_64_t__defined) */

#if	UseStaticTemplates
	OutP->object_name = object_nameTemplate;
#else	/* UseStaticTemplates */
#if __MigKernelSpecificCode
	OutP->object_name.disposition = 17;
#else
	OutP->object_name.disposition = 17;
#endif /* __MigKernelSpecificCode */
#if !(defined(KERNEL) && defined(__LP64__))
	OutP->object_name.pad1 = 0;
#endif
	OutP->object_name.pad2 = 0;
	OutP->object_name.type = MACH_MSG_PORT_DESCRIPTOR;
#if defined(KERNEL)
	OutP->object_name.pad_end = 0;
#endif
#endif	/* UseStaticTemplates */


	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	OutP->infoCnt = 10;
	if (In0P->infoCnt < OutP->infoCnt)
		OutP->infoCnt = In0P->infoCnt;

	RetCode = vm32_region_64(target_task, &In0P->address, &OutP->size, In0P->flavor, OutP->info, &OutP->infoCnt, &OutP->object_name.name);
	vm_map_deallocate(target_task);
	if (RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	OutP->address = In0P->address;
	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply) - 40) + (((4 * OutP->infoCnt)));

	OutP->Head.msgh_bits |= MACH_MSGH_BITS_COMPLEX;
	OutP->msgh_body.msgh_descriptor_count = 1;
	__AfterRcvRpc(3824, "region_64")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__make_memory_entry_64_t__defined)
#define __MIG_check__Request__make_memory_entry_64_t__defined

mig_internal kern_return_t __MIG_check__Request__make_memory_entry_64_t(__attribute__((__unused__)) __Request__make_memory_entry_64_t *In0P)
{

	typedef __Request__make_memory_entry_64_t __Request;
#if	__MigTypeCheck
	if (!(In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->msgh_body.msgh_descriptor_count != 1) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

#if	__MigTypeCheck
	if (In0P->parent_entry.type != MACH_MSG_PORT_DESCRIPTOR ||
	    In0P->parent_entry.disposition != 17)
		return MIG_TYPE_ERROR;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__make_memory_entry_64_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine make_memory_entry_64 */
mig_internal novalue _Xmake_memory_entry_64
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		/* start of the kernel processed data */
		mach_msg_body_t msgh_body;
		mach_msg_port_descriptor_t parent_entry;
		/* end of the kernel processed data */
		NDR_record_t NDR;
		memory_object_size_t size;
		memory_object_offset_t offset;
		vm_prot_t permission;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__make_memory_entry_64_t __Request;
	typedef __Reply__make_memory_entry_64_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__make_memory_entry_64_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__make_memory_entry_64_t__defined */

#if	__MigKernelSpecificCode
#if	UseStaticTemplates
	const static mach_msg_port_descriptor_t object_handleTemplate = {
		/* name = */		MACH_PORT_NULL,
		/* pad1 = */		0,
		/* pad2 = */		0,
		/* disp = */		17,
		/* type = */		MACH_MSG_PORT_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#else
#if	UseStaticTemplates
	const static mach_msg_port_descriptor_t object_handleTemplate = {
		/* name = */		MACH_PORT_NULL,
		/* pad1 = */		0,
		/* pad2 = */		0,
		/* disp = */		17,
		/* type = */		MACH_MSG_PORT_DESCRIPTOR,
	};
#endif	/* UseStaticTemplates */

#endif /* __MigKernelSpecificCode */
	kern_return_t RetCode;
	vm_map_t target_task;

	__DeclareRcvRpc(3825, "make_memory_entry_64")
	__BeforeRcvRpc(3825, "make_memory_entry_64")

#if	defined(__MIG_check__Request__make_memory_entry_64_t__defined)
	check_result = __MIG_check__Request__make_memory_entry_64_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__make_memory_entry_64_t__defined) */

#if	UseStaticTemplates
	OutP->object_handle = object_handleTemplate;
#else	/* UseStaticTemplates */
#if __MigKernelSpecificCode
	OutP->object_handle.disposition = 17;
#else
	OutP->object_handle.disposition = 17;
#endif /* __MigKernelSpecificCode */
#if !(defined(KERNEL) && defined(__LP64__))
	OutP->object_handle.pad1 = 0;
#endif
	OutP->object_handle.pad2 = 0;
	OutP->object_handle.type = MACH_MSG_PORT_DESCRIPTOR;
#if defined(KERNEL)
	OutP->object_handle.pad_end = 0;
#endif
#endif	/* UseStaticTemplates */


	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	RetCode = vm32_make_memory_entry_64(target_task, &In0P->size, In0P->offset, In0P->permission, &OutP->object_handle.name, null_conversion(In0P->parent_entry.name));
	vm_map_deallocate(target_task);
	if (RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, RetCode);
	}
#if	__MigKernelSpecificCode

	if (IP_VALID((ipc_port_t)In0P->parent_entry.name))
		ipc_port_release_send((ipc_port_t)In0P->parent_entry.name);
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	OutP->size = In0P->size;

	OutP->Head.msgh_bits |= MACH_MSGH_BITS_COMPLEX;
	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	OutP->msgh_body.msgh_descriptor_count = 1;
	__AfterRcvRpc(3825, "make_memory_entry_64")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__map_64_t__defined)
#define __MIG_check__Request__map_64_t__defined

mig_internal kern_return_t __MIG_check__Request__map_64_t(__attribute__((__unused__)) __Request__map_64_t *In0P)
{

	typedef __Request__map_64_t __Request;
#if	__MigTypeCheck
	if (!(In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->msgh_body.msgh_descriptor_count != 1) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

#if	__MigTypeCheck
	if (In0P->object.type != MACH_MSG_PORT_DESCRIPTOR ||
	    In0P->object.disposition != 17)
		return MIG_TYPE_ERROR;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__map_64_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine map_64 */
mig_internal novalue _Xmap_64
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		/* start of the kernel processed data */
		mach_msg_body_t msgh_body;
		mach_msg_port_descriptor_t object;
		/* end of the kernel processed data */
		NDR_record_t NDR;
		vm32_address_t address;
		vm32_size_t size;
		vm32_address_t mask;
		int flags;
		memory_object_offset_t offset;
		boolean_t copy;
		vm_prot_t cur_protection;
		vm_prot_t max_protection;
		vm_inherit_t inheritance;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__map_64_t __Request;
	typedef __Reply__map_64_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__map_64_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__map_64_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3826, "map_64")
	__BeforeRcvRpc(3826, "map_64")

#if	defined(__MIG_check__Request__map_64_t__defined)
	check_result = __MIG_check__Request__map_64_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__map_64_t__defined) */

	target_task = convert_port_entry_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32_map_64(target_task, &In0P->address, In0P->size, In0P->mask, In0P->flags, null_conversion(In0P->object.name), In0P->offset, In0P->copy, In0P->cur_protection, In0P->max_protection, In0P->inheritance);
	vm_map_deallocate(target_task);
	if (OutP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, OutP->RetCode);
	}
#if	__MigKernelSpecificCode

	if (IP_VALID((ipc_port_t)In0P->object.name))
		ipc_port_release_send((ipc_port_t)In0P->object.name);
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	OutP->address = In0P->address;

	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3826, "map_64")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request__purgable_control_t__defined)
#define __MIG_check__Request__purgable_control_t__defined

mig_internal kern_return_t __MIG_check__Request__purgable_control_t(__attribute__((__unused__)) __Request__purgable_control_t *In0P)
{

	typedef __Request__purgable_control_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request__purgable_control_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine purgable_control */
mig_internal novalue _Xpurgable_control
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		NDR_record_t NDR;
		vm32_address_t address;
		vm_purgable_t control;
		int state;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request__purgable_control_t __Request;
	typedef __Reply__purgable_control_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request__purgable_control_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request__purgable_control_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3830, "purgable_control")
	__BeforeRcvRpc(3830, "purgable_control")

#if	defined(__MIG_check__Request__purgable_control_t__defined)
	check_result = __MIG_check__Request__purgable_control_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request__purgable_control_t__defined) */

	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32_purgable_control(target_task, In0P->address, In0P->control, &In0P->state);
	vm_map_deallocate(target_task);
	if (OutP->RetCode != KERN_SUCCESS) {
		MIG_RETURN_ERROR(OutP, OutP->RetCode);
	}
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	OutP->state = In0P->state;

	OutP->Head.msgh_size = (mach_msg_size_t)(sizeof(Reply));
	__AfterRcvRpc(3830, "purgable_control")
}

#if ( __MigTypeCheck )
#if __MIG_check__Request__map_subsystem__
#if !defined(__MIG_check__Request___map_exec_lockdown_t__defined)
#define __MIG_check__Request___map_exec_lockdown_t__defined

mig_internal kern_return_t __MIG_check__Request___map_exec_lockdown_t(__attribute__((__unused__)) __Request___map_exec_lockdown_t *In0P)
{

	typedef __Request___map_exec_lockdown_t __Request;
#if	__MigTypeCheck
	if ((In0P->Head.msgh_bits & MACH_MSGH_BITS_COMPLEX) ||
	    (In0P->Head.msgh_size != (mach_msg_size_t)sizeof(__Request)))
		return MIG_BAD_ARGUMENTS;
#endif	/* __MigTypeCheck */

	return MACH_MSG_SUCCESS;
}
#endif /* !defined(__MIG_check__Request___map_exec_lockdown_t__defined) */
#endif /* __MIG_check__Request__map_subsystem__ */
#endif /* ( __MigTypeCheck ) */


/* Routine _map_exec_lockdown */
mig_internal novalue _X_map_exec_lockdown
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{

#ifdef  __MigPackStructs
#pragma pack(push, 4)
#endif
	typedef struct {
		mach_msg_header_t Head;
		mach_msg_trailer_t trailer;
	} Request __attribute__((unused));
#ifdef  __MigPackStructs
#pragma pack(pop)
#endif
	typedef __Request___map_exec_lockdown_t __Request;
	typedef __Reply___map_exec_lockdown_t Reply __attribute__((unused));

	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	Request *In0P = (Request *) InHeadP;
	Reply *OutP = (Reply *) OutHeadP;
#ifdef	__MIG_check__Request___map_exec_lockdown_t__defined
	kern_return_t check_result;
#endif	/* __MIG_check__Request___map_exec_lockdown_t__defined */

#if	__MigKernelSpecificCode
#else
#endif /* __MigKernelSpecificCode */
	vm_map_t target_task;

	__DeclareRcvRpc(3831, "_map_exec_lockdown")
	__BeforeRcvRpc(3831, "_map_exec_lockdown")

#if	defined(__MIG_check__Request___map_exec_lockdown_t__defined)
	check_result = __MIG_check__Request___map_exec_lockdown_t((__Request *)In0P);
	if (check_result != MACH_MSG_SUCCESS)
		{ MIG_RETURN_ERROR(OutP, check_result); }
#endif	/* defined(__MIG_check__Request___map_exec_lockdown_t__defined) */

	target_task = convert_port_to_map(In0P->Head.msgh_request_port);

	OutP->RetCode = vm32__map_exec_lockdown(target_task);
	vm_map_deallocate(target_task);
#if	__MigKernelSpecificCode
#endif /* __MigKernelSpecificCode */

	OutP->NDR = NDR_record;


	__AfterRcvRpc(3831, "_map_exec_lockdown")
}



/* Description of this subsystem, for use in direct RPC */
const struct vm32_map_subsystem vm32_map_subsystem = {
	map_server_routine,
	3800,
	3832,
	(mach_msg_size_t)sizeof(union __ReplyUnion__vm32_map_subsystem),
	(vm_address_t)0,
	{
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xregion, 7, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__region_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xallocate, 4, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__allocate_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xdeallocate, 3, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__deallocate_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xprotect, 5, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__protect_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xinherit, 4, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__inherit_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xread, 5, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__read_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xread_list, 3, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__read_list_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xwrite, 4, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__write_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xcopy, 4, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__copy_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xread_overwrite, 5, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__read_overwrite_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xmsync, 4, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__msync_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xbehavior_set, 4, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__behavior_set_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xmap, 11, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__map_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xmachine_attribute, 5, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__machine_attribute_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xremap, 11, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__remap_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _X_task_wire, 2, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply___task_wire_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xmake_memory_entry, 6, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__make_memory_entry_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xmap_page_query, 4, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__map_page_query_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xregion_info, 5, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__region_info_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xmapped_pages_info, 3, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__mapped_pages_info_t)},
		{0, 0, 0, 0, 0, 0},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xregion_recurse, 6, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__region_recurse_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xregion_recurse_64, 6, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__region_recurse_64_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xregion_info_64, 5, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__region_info_64_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xregion_64, 7, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__region_64_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xmake_memory_entry_64, 7, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__make_memory_entry_64_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xmap_64, 12, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__map_64_t)},
		{0, 0, 0, 0, 0, 0},
		{0, 0, 0, 0, 0, 0},
		{0, 0, 0, 0, 0, 0},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _Xpurgable_control, 4, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply__purgable_control_t)},
          { (mig_impl_routine_t) 0,
          (mig_stub_routine_t) _X_map_exec_lockdown, 1, 0, (routine_arg_descriptor_t)0, (mach_msg_size_t)sizeof(__Reply___map_exec_lockdown_t)},
	}
};

mig_external boolean_t map_server
	(mach_msg_header_t *InHeadP, mach_msg_header_t *OutHeadP)
{
	/*
	 * typedef struct {
	 * 	mach_msg_header_t Head;
	 * 	NDR_record_t NDR;
	 * 	kern_return_t RetCode;
	 * } mig_reply_error_t;
	 */

	mig_routine_t routine;

	OutHeadP->msgh_bits = MACH_MSGH_BITS(MACH_MSGH_BITS_REPLY(InHeadP->msgh_bits), 0);
	OutHeadP->msgh_remote_port = InHeadP->msgh_reply_port;
	/* Minimal size: routine() will update it if different */
	OutHeadP->msgh_size = (mach_msg_size_t)sizeof(mig_reply_error_t);
	OutHeadP->msgh_local_port = MACH_PORT_NULL;
	OutHeadP->msgh_id = InHeadP->msgh_id + 100;
	OutHeadP->msgh_reserved = 0;

	if ((InHeadP->msgh_id > 3831) || (InHeadP->msgh_id < 3800) ||
	    ((routine = vm32_map_subsystem.routine[InHeadP->msgh_id - 3800].stub_routine) == 0)) {
		((mig_reply_error_t *)OutHeadP)->NDR = NDR_record;
		((mig_reply_error_t *)OutHeadP)->RetCode = MIG_BAD_ID;
		return FALSE;
	}
	(*routine) (InHeadP, OutHeadP);
	return TRUE;
}

mig_external mig_routine_t map_server_routine
	(mach_msg_header_t *InHeadP)
{
	int msgh_id;

	msgh_id = InHeadP->msgh_id - 3800;

	if ((msgh_id > 31) || (msgh_id < 0))
		return 0;

	return vm32_map_subsystem.routine[msgh_id].stub_routine;
}
